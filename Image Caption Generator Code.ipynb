{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Import Necessary Packages"],"metadata":{"id":"EhGhWHhc896S"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ynBrVs11NFl"},"outputs":[],"source":["import string\n","import numpy as np\n","from PIL import Image\n","import os\n","from pickle import dump, load\n","import numpy as np\n","\n","from keras.applications.xception import Xception, preprocess_input\n","from keras.preprocessing.image import load_img, img_to_array\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","#from keras.layers.merge import add\n","from keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n","from keras.models import Model, load_model\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","from tensorflow.keras.callbacks import Callback\n","\n","from tqdm.notebook import tqdm\n","tqdm.pandas()"]},{"cell_type":"markdown","source":["## Data Cleaning Functions"],"metadata":{"id":"VmcDvAxN9GoF"}},{"cell_type":"code","source":["# Read Image Captions from files in Flickr8k_text & organize into a dictionary\n","def all_img_captions(filename):\n","    with open(filename, 'r') as f:\n","        file = f.read()\n","    captions = file.split('\\n')\n","    descriptions = {}\n","    for caption in captions[:-1]:\n","        img, caption = caption.split('\\t')\n","        if img[:-2] not in descriptions:\n","            descriptions[img[:-2]] = [caption]\n","        else:\n","            descriptions[img[:-2]].append(caption)\n","    return descriptions"],"metadata":{"id":"CI-KBtwY1nwe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Caption cleaning - lower casing, removing puntuations and words containing numbers\n","def cleaning_text(captions):\n","    table = str.maketrans('','',string.punctuation)\n","    for img,caps in captions.items():\n","        for i,img_caption in enumerate(caps):\n","\n","            img_caption.replace(\"-\",\" \")\n","            desc = img_caption.split()\n","\n","            #converts to lower case\n","            desc = [word.lower() for word in desc]\n","            #remove punctuation from each token\n","            desc = [word.translate(table) for word in desc]\n","            #remove hanging 's and a\n","            desc = [word for word in desc if(len(word)>1)]\n","            #remove tokens with numbers in them\n","            desc = [word for word in desc if(word.isalpha())]\n","            #convert back to string\n","\n","            img_caption = ' '.join(desc)\n","            captions[img][i]= img_caption\n","    return captions\n","\n","\n","# Caption Cleaning - keep unique word from the 5 captions\n","def text_vocabulary(descriptions):\n","    # build vocabulary of all unique words\n","    vocab = set()\n","\n","    for key in descriptions.keys():\n","        [vocab.update(d.split()) for d in descriptions[key]]\n","\n","    return vocab\n","\n","\n","# All descriptions in one file\n","def save_descriptions(descriptions, filename):\n","    lines = list()\n","    for key, desc_list in descriptions.items():\n","        for desc in desc_list:\n","            lines.append(key + '\\t' + desc )\n","    data = \"\\n\".join(lines)\n","    file = open(filename,\"w\")\n","    file.write(data)\n","    file.close()"],"metadata":{"id":"4hGAtQxA2jrt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Process images from the Flicker8k_Dataset\n","# extract features using Xception, ResNet50, and VGG16\n","from keras.applications.resnet50 import ResNet50, preprocess_input\n","\n","def Xception_extract_features(directory):\n","        model = Xception( include_top=False, pooling='avg' )\n","        features = {}\n","        for img in tqdm(os.listdir(directory)):\n","            filename = directory + \"/\" + img\n","            image = Image.open(filename)\n","            image = image.resize((299,299))\n","            image = np.expand_dims(image, axis=0)\n","            #image = preprocess_input(image)\n","            image = image/127.5\n","            image = image - 1.0\n","\n","            feature = model.predict(image)\n","            features[img] = feature\n","        return features\n","\n","def ResNet50_extract_features(directory):\n","        model = ResNet50(include_top=False, pooling='avg', weights='imagenet')\n","        features = {}\n","        for img in tqdm(os.listdir(directory)):\n","          filename = os.path.join(directory, img)\n","          # Open and resize image to 224x224 (required input size for ResNet50)\n","          image = Image.open(filename).resize((224, 224))\n","          # Convert the image to a numpy array and add a batch dimension\n","          image = np.expand_dims(np.array(image), axis=0)\n","          # Preprocess the image using the appropriate preprocess_input function for ResNet50\n","          image = preprocess_input(image)\n","          # Predict features with the model\n","          feature = model.predict(image)\n","          # Store the feature vector in a dictionary with the image filename as the key\n","          features[img] = feature.flatten()\n","        return features\n","\n","from keras.applications.vgg16 import VGG16, preprocess_input\n","\n","def VGG16_extract_features(directory):\n","    # Load VGG16 model, exclude the top fully connected layers, and use average pooling\n","    model = VGG16(include_top=False, pooling='avg', weights='imagenet')\n","    features = {}\n","    # Iterate through all images in the specified directory\n","    for img in tqdm(os.listdir(directory)):\n","        filename = os.path.join(directory, img)\n","        # Open and resize image to 224x224 (required input size for VGG16)\n","        image = Image.open(filename).resize((224, 224))\n","        image = np.expand_dims(np.array(image), axis=0)\n","        image = preprocess_input(image)\n","        feature = model.predict(image)\n","        features[img] = feature.flatten()\n","\n","    return features"],"metadata":{"id":"A4R36tZx2u_M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prepare Text Data & Image Data"],"metadata":{"id":"g2P4SSDbFCxc"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HcQ3_7MA5fy-","executionInfo":{"status":"ok","timestamp":1714771596669,"user_tz":240,"elapsed":25132,"user":{"displayName":"Fangzhou Yuan","userId":"02366347569993012260"}},"outputId":"90acaba9-e79c-41e7-f70c-c56c4a4f6957"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#Preparing our text data\n","#Shared folder need to add a shorcut to MyDrive\n","dataset_text = \"/content/drive/MyDrive/DL_Project_2024/Flickr8k_text\"\n","dataset_images = \"/content/drive/MyDrive/DL_Project_2024/Flicker8k_Dataset\"\n","\n","filename = dataset_text + \"/\" + \"Flickr8k.token.txt\"\n","#loading the file that contains all data\n","#mapping them into descriptions dictionary img to 5 captions\n","descriptions = all_img_captions(filename)\n","print(\"Length of descriptions =\" ,len(descriptions))\n","\n","#cleaning the descriptions\n","clean_descriptions = cleaning_text(descriptions)\n","\n","#building vocabulary\n","vocabulary = text_vocabulary(clean_descriptions)\n","print(\"Length of vocabulary = \", len(vocabulary))\n","\n","#saving each description to file\n","save_descriptions(clean_descriptions, \"/content/drive/MyDrive/DL_Project_2024/temp/descriptions.txt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fRsgtoel2tEx","executionInfo":{"status":"ok","timestamp":1714790317098,"user_tz":240,"elapsed":877,"user":{"displayName":"Fangzhou Yuan","userId":"02366347569993012260"}},"outputId":"bf72e12e-7d6b-492f-98c4-16d7ca16bcea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Length of descriptions = 8092\n","Length of vocabulary =  8763\n"]}]},{"cell_type":"code","source":["#Preparing our Image data\n","#2048 feature vector\n","features = Xception_extract_features(dataset_images)\n","dump(features, open(\"/content/drive/MyDrive/DL_Project_2024/temp/features.p\",\"wb\"))"],"metadata":{"id":"t4fdKloY8UlP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load Processed Data"],"metadata":{"id":"vjVSMROkGAT7"}},{"cell_type":"code","source":["features = load(open(\"/content/drive/MyDrive/DL_Project_2024/temp/features.p\",\"rb\"))"],"metadata":{"id":"FtSZJDbh8aUN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#load the data\n","def load_photos(filename):\n","    with open(filename, 'r') as f:\n","        file = f.read()\n","    photos = file.split(\"\\n\")[:-1]\n","    return photos\n","\n","#loading clean_descriptions\n","def load_clean_descriptions(filename, photos):\n","    with open(filename, 'r') as f:\n","        file = f.read()\n","    descriptions = {}\n","    for line in file.split(\"\\n\"):\n","\n","        words = line.split()\n","        if len(words) < 1:\n","            continue\n","\n","        image, image_caption = words[0], words[1:]\n","\n","        if image in photos:\n","            if image not in descriptions:\n","                descriptions[image] = []\n","            desc = '<start> ' + \" \".join(image_caption) + ' <end>'\n","            descriptions[image].append(desc)\n","\n","    return descriptions\n","\n","def load_features(photos):\n","    #loading all features\n","    all_features = load(open(\"/content/drive/MyDrive/DL_Project_2024/temp/features.p\",\"rb\"))\n","    #selecting only needed features\n","    features = {k:all_features[k] for k in photos}\n","    return features\n"],"metadata":{"id":"yoLztbhE8aWX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filename = dataset_text + \"/\" + \"Flickr_8k.trainImages.txt\"\n","\n","#train = loading_data(filename)\n","train_imgs = load_photos(filename)\n","train_descriptions = load_clean_descriptions(\"/content/drive/MyDrive/DL_Project_2024/temp/descriptions.txt\", train_imgs)\n","train_features = load_features(train_imgs)"],"metadata":{"id":"BON9FzWx8aYU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_imgs = train_imgs + load_photos(dataset_text + \"/\" + \"Flickr_8k.testImages.txt\")\n","all_descriptions = load_clean_descriptions(\"/content/drive/MyDrive/DL_Project_2024/temp/descriptions.txt\", all_imgs)"],"metadata":{"id":"iMW0sbs6fxMK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#converting dictionary to clean list of descriptions\n","def dict_to_list(descriptions):\n","    all_desc = []\n","    for key in descriptions.keys():\n","        [all_desc.append(d) for d in descriptions[key]]\n","    return all_desc\n","\n","#creating tokenizer class\n","#this will vectorise text corpus\n","#each integer will represent token in dictionary\n","\n","from keras.preprocessing.text import Tokenizer\n","\n","def create_tokenizer(descriptions):\n","    desc_list = dict_to_list(descriptions)\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(desc_list)\n","    return tokenizer"],"metadata":{"id":"ZnOroajf8eb0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preparing text data for training deep learning models\n","\n","# give each word a index, and store that into tokenizer.p pickle file\n","tokenizer = create_tokenizer(train_descriptions)\n","dump(tokenizer, open('/content/drive/MyDrive/DL_Project_2024/temp/tokenizer.p', 'wb'))\n","vocab_size = len(tokenizer.word_index) + 1\n","vocab_size"],"metadata":{"id":"wBK0whza8eeK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714771617035,"user_tz":240,"elapsed":1561,"user":{"displayName":"Fangzhou Yuan","userId":"02366347569993012260"}},"outputId":"8631e165-6dac-4fa7-9c65-f9ec77d2392d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7577"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["#calculate maximum length of descriptions\n","def max_length(descriptions):\n","    desc_list = dict_to_list(descriptions)\n","    return max(len(d.split()) for d in desc_list)\n","\n","max_length = max_length(descriptions)\n","max_length\n","# This will be the input shape"],"metadata":{"id":"d82urXpO8egf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714771624059,"user_tz":240,"elapsed":341,"user":{"displayName":"Fangzhou Yuan","userId":"02366347569993012260"}},"outputId":"73e4270b-4b92-4838-83ff-935808e5a37b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["32"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["features['1000268201_693b08cb0e.jpg'][0]"],"metadata":{"id":"aivoIqEY8eiu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714513635563,"user_tz":240,"elapsed":10,"user":{"displayName":"Fangzhou Yuan","userId":"02366347569993012260"}},"outputId":"9798707b-118d-425c-b85b-e6337889077d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.4734095 , 0.01730889, 0.07334236, ..., 0.08557957, 0.02102294,\n","       0.2376553 ], dtype=float32)"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["## Define Our Model"],"metadata":{"id":"b4SklKYFGQ24"}},{"cell_type":"code","source":["# Define the model\n","\n","#1 Photo feature extractor - we extracted features from pretrained model Xception.\n","#2 Sequence processor - word embedding layer that handles text, followed by LSTM\n","#3 Decoder - Both 1 and 2 model produce fixed length vector. They are merged together and processed by dense layer to make final prediction"],"metadata":{"id":"vuWZcl-Y8kg-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create input-output sequence pairs from the image description.\n","\n","#data generator, used by model.fit_generator()\n","def data_generator(descriptions, features, tokenizer, max_length):\n","    while 1:\n","        for key, description_list in descriptions.items():\n","            #retrieve photo features\n","            feature = features[key][0]\n","            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)\n","            yield [[input_image, input_sequence], output_word]\n","\n","def create_sequences(tokenizer, max_length, desc_list, feature):\n","    X1, X2, y = list(), list(), list()\n","    # walk through each description for the image\n","    for desc in desc_list:\n","        # encode the sequence\n","        seq = tokenizer.texts_to_sequences([desc])[0]\n","        # split one sequence into multiple X,y pairs\n","        for i in range(1, len(seq)):\n","            # split into input and output pair\n","            in_seq, out_seq = seq[:i], seq[i]\n","            # pad input sequence\n","            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","            # encode output sequence\n","            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","            # store\n","            X1.append(feature)\n","            X2.append(in_seq)\n","            y.append(out_seq)\n","    return np.array(X1), np.array(X2), np.array(y)"],"metadata":{"id":"HwgqiWC38kjN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["[a,b],c = next(data_generator(train_descriptions, features, tokenizer, max_length))\n","a.shape, b.shape, c.shape"],"metadata":{"id":"alhKo3gh8kl2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714513635563,"user_tz":240,"elapsed":8,"user":{"displayName":"Fangzhou Yuan","userId":"02366347569993012260"}},"outputId":"00e55d55-3533-421f-d6ae-e1f215776814"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((47, 2048), (47, 32), (47, 7577))"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["from keras.utils import plot_model\n","\n","# define the captioning model\n","def define_model(vocab_size, max_length):\n","\n","    # features from the CNN model squeezed from 2048 to 256 nodes\n","    inputs1 = Input(shape=(2048,))\n","    fe1 = Dropout(0.5)(inputs1)\n","    fe2 = Dense(256, activation='relu')(fe1)\n","\n","    # LSTM sequence model\n","    inputs2 = Input(shape=(max_length,))\n","    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n","    se2 = Dropout(0.5)(se1)\n","    se3 = LSTM(256)(se2)\n","\n","    # Merging both models\n","    decoder1 = add([fe2, se3])\n","    decoder2 = Dense(256, activation='relu')(decoder1)\n","    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n","\n","    # tie it together [image, seq] [word]\n","    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","    model.compile(loss='categorical_crossentropy', optimizer='adam')\n","\n","    # summarize model\n","    print(model.summary())\n","    plot_model(model, to_file='/content/drive/MyDrive/DL_Project_2024/temp/model.png', show_shapes=True)\n","\n","    return model\n"],"metadata":{"id":"mc-o1jag82G9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"la2pINOzGVGP"}},{"cell_type":"code","source":["# train our model\n","print('Dataset: ', len(train_imgs))\n","print('Descriptions: train=', len(train_descriptions))\n","print('Photos: train=', len(train_features))\n","print('Vocabulary Size:', vocab_size)\n","print('Description Length: ', max_length)\n","\n","model = define_model(vocab_size, max_length)\n","epochs = 10\n","steps = len(train_descriptions)\n","# # making a directory models to save our models\n","# os.mkdir(\"/content/drive/MyDrive/DL_Project_2024/temp/models\")\n","for i in range(epochs):\n","    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n","    model.fit(generator, epochs=1, steps_per_epoch= steps, verbose=1)\n","    model.save(\"/content/drive/MyDrive/DL_Project_2024/temp/models/model_\" + str(i) + \".h5\")"],"metadata":{"id":"Eg7h8xuU82I6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"14ed77c2-ec62-43c1-e780-add26518013c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset:  6000\n","Descriptions: train= 6000\n","Photos: train= 6000\n","Vocabulary Size: 7577\n","Description Length:  32\n","Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_2 (InputLayer)        [(None, 32)]                 0         []                            \n","                                                                                                  \n"," input_1 (InputLayer)        [(None, 2048)]               0         []                            \n","                                                                                                  \n"," embedding (Embedding)       (None, 32, 256)              1939712   ['input_2[0][0]']             \n","                                                                                                  \n"," dropout (Dropout)           (None, 2048)                 0         ['input_1[0][0]']             \n","                                                                                                  \n"," dropout_1 (Dropout)         (None, 32, 256)              0         ['embedding[0][0]']           \n","                                                                                                  \n"," dense (Dense)               (None, 256)                  524544    ['dropout[0][0]']             \n","                                                                                                  \n"," lstm (LSTM)                 (None, 256)                  525312    ['dropout_1[0][0]']           \n","                                                                                                  \n"," add (Add)                   (None, 256)                  0         ['dense[0][0]',               \n","                                                                     'lstm[0][0]']                \n","                                                                                                  \n"," dense_1 (Dense)             (None, 256)                  65792     ['add[0][0]']                 \n","                                                                                                  \n"," dense_2 (Dense)             (None, 7577)                 1947289   ['dense_1[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 5002649 (19.08 MB)\n","Trainable params: 5002649 (19.08 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n","None\n","6000/6000 [==============================] - 543s 88ms/step - loss: 4.5281\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["6000/6000 [==============================] - 526s 88ms/step - loss: 3.6845\n","6000/6000 [==============================] - 543s 91ms/step - loss: 3.3950\n","6000/6000 [==============================] - 531s 89ms/step - loss: 3.2229\n","6000/6000 [==============================] - 535s 89ms/step - loss: 3.1012\n","1003/6000 [====>.........................] - ETA: 7:23 - loss: 3.0513"]}]},{"cell_type":"code","source":["from keras.models import load_model\n","import os\n","\n","# Load the previously trained model\n","model = load_model('/content/drive/MyDrive/DL_Project_2024/temp/models/model_12.h5')\n","\n","# Compile the model with accuracy metric\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Set the number of epochs for additional training\n","epochs = 8\n","steps = len(train_descriptions)\n","\n","# Train the model further\n","for i in range(epochs):\n","    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n","    history = model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n","    print(f\"Epoch {i+1}/{epochs}, Accuracy: {history.history['accuracy'][-1]}\")\n","    # Save the model after each epoch\n","    model.save(\"/content/drive/MyDrive/DL_Project_2024/temp/models/model_continued_\" + str(i) + \".h5\")\n"],"metadata":{"id":"sZyAYuT-8knz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714781133978,"user_tz":240,"elapsed":4395053,"user":{"displayName":"Fangzhou Yuan","userId":"02366347569993012260"}},"outputId":"ee9e7a10-10fc-4afb-a2cf-3ad2350a8bba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["6000/6000 [==============================] - 548s 91ms/step - loss: 2.8233 - accuracy: 0.3611\n","Epoch 1/8, Accuracy: 0.3611343204975128\n","6000/6000 [==============================] - 530s 88ms/step - loss: 2.7778 - accuracy: 0.3636\n","Epoch 2/8, Accuracy: 0.363608181476593\n","6000/6000 [==============================] - 554s 92ms/step - loss: 2.7483 - accuracy: 0.3670\n","Epoch 3/8, Accuracy: 0.3669893443584442\n","6000/6000 [==============================] - 530s 88ms/step - loss: 2.7245 - accuracy: 0.3676\n","Epoch 4/8, Accuracy: 0.367609441280365\n","6000/6000 [==============================] - 530s 88ms/step - loss: 2.7118 - accuracy: 0.3697\n","Epoch 5/8, Accuracy: 0.36972755193710327\n","6000/6000 [==============================] - 549s 91ms/step - loss: 2.6929 - accuracy: 0.3720\n","Epoch 6/8, Accuracy: 0.3719729483127594\n","6000/6000 [==============================] - 529s 88ms/step - loss: 2.6761 - accuracy: 0.3735\n","Epoch 7/8, Accuracy: 0.37351667881011963\n","6000/6000 [==============================] - 530s 88ms/step - loss: 2.6653 - accuracy: 0.3746\n","Epoch 8/8, Accuracy: 0.37462306022644043\n"]}]},{"cell_type":"markdown","source":["## Training with BLEU score"],"metadata":{"id":"alGLO3DtU8Gf"}},{"cell_type":"code","source":["class BLEUScoreCallback(Callback):\n","    def __init__(self, features, descriptions, tokenizer, max_length):\n","        self.features = features\n","        self.descriptions = descriptions\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.stemmer = PorterStemmer()\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        actual, predicted = [], []\n","        for key, desc_list in self.descriptions.items():\n","            image_feature = self.features[key][0]\n","            generated_caption = generate_caption(self.model, image_feature, self.tokenizer, self.max_length)\n","            # Stemming the generated caption\n","            stemmed_generated = ' '.join([self.stemmer.stem(word) for word in word_tokenize(generated_caption)])\n","\n","            # Stemming the reference captions\n","            stemmed_references = []\n","            for ref in desc_list:\n","                stemmed_ref = [self.stemmer.stem(word) for word in word_tokenize(ref)]\n","                stemmed_references.append(stemmed_ref)\n","            actual.append(stemmed_references)\n","            predicted.append(stemmed_generated.split())\n","\n","        # compute the BLEU score\n","        bleu_score = corpus_bleu(actual, predicted, smoothing_function=SmoothingFunction().method1)\n","        print(f'Epoch {epoch + 1}: BLEU Score = {bleu_score:.4f}')\n","        if logs is not None:\n","            logs['val_bleu'] = bleu_score\n"],"metadata":{"id":"D62iv_i0U_mX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Dataset: ', len(train_imgs))\n","print('Descriptions: train=', len(train_descriptions))\n","print('Photos: train=', len(train_features))\n","print('Vocabulary Size:', vocab_size)\n","print('Description Length: ', max_length)\n","\n","# Initialize the model\n","model = define_model(vocab_size, max_length)\n","\n","# BLEU score callback initialization\n","bleu_callback = BLEUScoreCallback(train_features, train_descriptions, tokenizer, max_length)\n","\n","# Fit the model\n","epochs = 10\n","steps = len(train_descriptions)\n","generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n","\n","# Train with BLEU score evaluation\n","model.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1, callbacks=[bleu_callback])"],"metadata":{"id":"TYvI7JkFVc_K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Image Captions Demo"],"metadata":{"id":"_7j34blGUvbz"}},{"cell_type":"code","source":["import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from keras.models import load_model\n","from keras.applications.xception import Xception\n","from keras.preprocessing.sequence import pad_sequences\n","from pickle import load\n","import os\n","import random\n","\n","def extract_features(filename, model):\n","    try:\n","        image = Image.open(filename)\n","    except:\n","        print(\"ERROR: Couldn't open image! Make sure the image path and extension is correct\")\n","        return None\n","    image = image.resize((299, 299))\n","    image = np.array(image)\n","    if image.shape[-1] == 4:  # for images with 4 channels, convert to 3 channels\n","        image = image[..., :3]\n","    image = np.expand_dims(image, axis=0)\n","    image = image / 127.5 - 1.0\n","    feature = model.predict(image)\n","    return feature\n","\n","def word_for_id(integer, tokenizer):\n","    for word, index in tokenizer.word_index.items():\n","        if index == integer:\n","            return word\n","    return None\n","\n","def generate_desc(model, tokenizer, photo, max_length):\n","    in_text = 'start'\n","    for i in range(max_length):\n","        sequence = tokenizer.texts_to_sequences([in_text])[0]\n","        sequence = pad_sequences([sequence], maxlen=max_length)\n","        pred = model.predict([photo, sequence], verbose=0)\n","        pred = np.argmax(pred)\n","        word = word_for_id(pred, tokenizer)\n","        if word is None:\n","            break\n","        in_text += ' ' + word\n","        if word == 'end':\n","            break\n","    words = in_text.split()\n","    in_text = ' '.join(words[1:-1])\n","    return in_text\n","\n","# Directory containing images\n","image_dir = '/content/drive/MyDrive/DL_Project_2024/Flicker8k_Dataset'\n","\n","ps = PorterStemmer()\n","\n","for _ in range(10):\n","    # Randomly choose an image\n","    # image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n","    random_image = random.choice(all_imgs)\n","    img_path = os.path.join(image_dir, random_image)\n","    print(random_image)\n","    # img_path = os.path.join(image_dir, '19212715_20476497a3.jpg')\n","    # img_path = os.path.join(image_dir, '12830823_87d2654e31.jpg')\n","\n","    # Load necessary models and tokenizer\n","    max_length = 32\n","    tokenizer = load(open(\"/content/drive/MyDrive/DL_Project_2024/temp/tokenizer.p\", \"rb\"))\n","    model = load_model('/content/drive/MyDrive/DL_Project_2024/temp/models/model_20.h5')\n","    xception_model = Xception(include_top=False, pooling=\"avg\")\n","\n","    # Extract features and generate description\n","    photo = extract_features(img_path, xception_model)\n","    if photo is not None:\n","        img = Image.open(img_path)\n","\n","        # true caption\n","        reference_captions = all_descriptions[random_image]\n","        true_caption_stems = []\n","        for reference_caption in reference_captions:\n","            true_caption_tokens = word_tokenize(reference_caption)\n","            true_caption_stem = [ps.stem(word) for word in true_caption_tokens]\n","            true_caption_stem = ' '.join(true_caption_stem)\n","            true_caption_stems.append(true_caption_stem)\n","        print(\"True Caption: \", reference_captions)\n","\n","        # generated caption\n","        description = generate_desc(model, tokenizer, photo, max_length)\n","        generated = word_tokenize(description)\n","        generated_stem = [ps.stem(word) for word in generated]\n","        generated_stem = ' '.join(generated_stem)\n","        print(\"Generated Caption: \", ' '.join(generated))\n","\n","        # Compute BLEU score\n","        chencherry = SmoothingFunction()\n","        bleu_score = corpus_bleu([true_caption_stems], [generated_stem], smoothing_function=chencherry.method1)\n","        print(\"Score: \", bleu_score)\n","\n","        plt.imshow(img)\n","        plt.show()\n","    else:\n","        print(\"Failed to extract features.\")"],"metadata":{"id":"LGdbR4ZVtBP2","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1hdMeqxFGEwbvihEWdrd-CWgCq3O-FYPw"},"outputId":"91807c50-5541-47ec-ce1a-11be548ad2f8","executionInfo":{"status":"ok","timestamp":1714792360331,"user_tz":240,"elapsed":80459,"user":{"displayName":"Fangzhou Yuan","userId":"02366347569993012260"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"EuCYAYs3-E1G"}}]}